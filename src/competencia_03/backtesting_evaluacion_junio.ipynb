{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Configuraciones Generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Librerías.\n",
    "%run \"../librerias.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Constantes.\n",
    "#a. Constantes generales.\n",
    "%run \"../constantes.ipynb\"\n",
    "\n",
    "#b. Constantes a definir por el usuario.\n",
    "#Información sobre rango temporal del modelo.\n",
    "cantidad_meses_train = \"all\"\n",
    "ventana = 1\n",
    "#Meses de train y test.\n",
    "mes_train = mes_train_all_menos_2_sin_rotas_backtesting\n",
    "mes_test = mes_test_backtesting\n",
    "\n",
    "#i. Modelo 1.\n",
    "#1. Dataset de lectura (post-feature engineering y undersampleado).\n",
    "dataset_con_fe_m1 = dataset_file_fe_1_all_undersampleado\n",
    "dataset_con_test_m1 = dataset_file_fe_all_1\n",
    "#2. Ruta de la BBDD donde se almacenan los hiperparámetros óptimos post-Optuna.\n",
    "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
    "study_name_m1 = f\"exp_lgbm_comp3_{cantidad_meses_train}_{ventana}_undersampling_backtesting\"\n",
    "\n",
    "#ii. Modelo 2.\n",
    "#1. Dataset de lectura (post-feature engineering y undersampleado).\n",
    "dataset_con_fe_m2 = dataset_file_fe_1_all_undersampleado_lagsdelta\n",
    "dataset_con_test_m2 = dataset_file_fe_all_1_lagsdelta\n",
    "#2. Ruta de la BBDD donde se almacenan los hiperparámetros óptimos post-Optuna.\n",
    "study_name_m2 = f\"exp_lgbm_comp3_{cantidad_meses_train}_{ventana}_undersampling_backtesting_lagsdelta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Lectura de datos.\n",
    "#i. Modelo 1.\n",
    "#a. Train.\n",
    "data_m1 = pd.read_parquet(dataset_con_fe_m1)\n",
    "#b. Test.\n",
    "test_m1 = pd.read_parquet(dataset_con_test_m1)\n",
    "\n",
    "#ii. Modelo 2.\n",
    "#a. Train.\n",
    "data_m2 = pd.read_parquet(dataset_con_fe_m2)\n",
    "#b. Test.\n",
    "test_m2 = pd.read_parquet(dataset_con_test_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Importo el estudio de Optuna.\n",
    "#i. Modelo 1.\n",
    "study_m1 = optuna.load_study(\n",
    "    study_name=study_name_m1,\n",
    "    storage=storage_name\n",
    "    )\n",
    "\n",
    "#ii. Modelo 2.\n",
    "study_m2 = optuna.load_study(\n",
    "    study_name=study_name_m2,\n",
    "    storage=storage_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Pre-procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Concateno Train y Test.\n",
    "#i. Modelo 1.\n",
    "#a. Filtro el mes de interés en Test.\n",
    "test_m1 = test_m1[test_m1[\"foto_mes\"] == mes_test]\n",
    "#b. Borro la columna de clase binaria en train para poder concatenar.\n",
    "data_m1.drop([\"clase_binaria\"],axis=1,inplace=True)\n",
    "#c. Concateno.\n",
    "data_m1 = pd.concat([data_m1,test_m1],axis=0)\n",
    "\n",
    "#ii. Modelo 2.\n",
    "#a. Filtro el mes de interés en Test.\n",
    "test_m2 = test_m2[test_m2[\"foto_mes\"] == mes_test]\n",
    "#b. Borro la columna de clase binaria en train para poder concatenar.\n",
    "data_m2.drop([\"clase_binaria\"],axis=1,inplace=True)\n",
    "#c. Concateno.\n",
    "data_m2 = pd.concat([data_m2,test_m2],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Pequeño pre-procesamiento sobre los datos.\n",
    "#i. Modelo 1.\n",
    "#a. Cambio tipos de datos (Me lo toma como tipo de dato \"object\"...)\n",
    "data_m1['ctrx_quarter_normalizado'] = data_m1['ctrx_quarter_normalizado'].astype(float)\n",
    "\n",
    "#b. Pesos y reclusterización.\n",
    "data_m1['clase_peso'] = 1.0\n",
    "\n",
    "data_m1.loc[data_m1['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "data_m1.loc[data_m1['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "data_m1['clase_binaria'] = 0\n",
    "data_m1['clase_binaria'] = np.where(data_m1['clase_ternaria'] == 'CONTINUA', 0, 1)\n",
    "\n",
    "#ii. Modelo 2.\n",
    "#a. Cambio tipos de datos (Me lo toma como tipo de dato \"object\"...)\n",
    "data_m2['ctrx_quarter_normalizado'] = data_m2['ctrx_quarter_normalizado'].astype(float)\n",
    "\n",
    "#b. Pesos y reclusterización.\n",
    "data_m2['clase_peso'] = 1.0\n",
    "\n",
    "data_m2.loc[data_m2['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "data_m2.loc[data_m2['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "data_m2['clase_binaria'] = 0\n",
    "data_m2['clase_binaria'] = np.where(data_m2['clase_ternaria'] == 'CONTINUA', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Dividimos entre conjuntos de datos.\n",
    "#i. Modelo 1.\n",
    "#a. Datos para entrenar todo el modelo final para Kaggle.\n",
    "train_data_m1 = data_m1[data_m1['foto_mes'].isin(mes_train)]\n",
    "test_data_m1 = data_m1[data_m1['foto_mes'] == mes_test]\n",
    "\n",
    "X_train_m1 = train_data_m1.drop(['clase_ternaria', 'clase_peso','clase_binaria'], axis=1)\n",
    "y_train_binaria_m1 = train_data_m1['clase_binaria']\n",
    "w_train_m1 = train_data_m1['clase_peso']\n",
    "\n",
    "#b. Datos de Test (a predecir).\n",
    "X_test_m1 = test_data_m1.drop(['clase_ternaria', 'clase_peso','clase_binaria'], axis=1)\n",
    "y_test_binaria_m1 = test_data_m1['clase_binaria']\n",
    "\n",
    "#ii. Modelo 2.\n",
    "#a. Datos para entrenar todo el modelo final para Kaggle.\n",
    "train_data_m2 = data_m2[data_m2['foto_mes'].isin(mes_train)]\n",
    "test_data_m2 = data_m2[data_m2['foto_mes'] == mes_test]\n",
    "\n",
    "X_train_m2 = train_data_m2.drop(['clase_ternaria', 'clase_peso','clase_binaria'], axis=1)\n",
    "y_train_binaria_m2 = train_data_m2['clase_binaria']\n",
    "w_train_m2 = train_data_m2['clase_peso']\n",
    "\n",
    "#b. Datos de Test (a predecir).\n",
    "X_test_m2 = test_data_m2.drop(['clase_ternaria', 'clase_peso','clase_binaria'], axis=1)\n",
    "y_test_binaria_m2 = test_data_m2['clase_binaria']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Entrenamiento simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. Modelo 1.\n",
    "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'first_metric_only': True,\n",
    "    'boost_from_average': True,\n",
    "    'feature_pre_filter': False,\n",
    "    'max_bin': 31,\n",
    "    'num_leaves': study_m1.best_trial.params['num_leaves'],\n",
    "    'learning_rate': study_m1.best_trial.params['learning_rate'],\n",
    "    'min_data_in_leaf': study_m1.best_trial.params['min_data_in_leaf'],\n",
    "    'feature_fraction': study_m1.best_trial.params['feature_fraction'],\n",
    "    'bagging_fraction': study_m1.best_trial.params['bagging_fraction'],\n",
    "    'seed': semillas[0],\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "train_data_m1 = lgb.Dataset(X_train_m1,\n",
    "                          label=y_train_binaria_m1,\n",
    "                          weight=w_train_m1)\n",
    "\n",
    "modelo_1 = lgb.train(params,\n",
    "                  train_data_m1,\n",
    "                  num_boost_round=best_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Modelo 2.\n",
    "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'first_metric_only': True,\n",
    "    'boost_from_average': True,\n",
    "    'feature_pre_filter': False,\n",
    "    'max_bin': 31,\n",
    "    'num_leaves': study_m2.best_trial.params['num_leaves'],\n",
    "    'learning_rate': study_m2.best_trial.params['learning_rate'],\n",
    "    'min_data_in_leaf': study_m2.best_trial.params['min_data_in_leaf'],\n",
    "    'feature_fraction': study_m2.best_trial.params['feature_fraction'],\n",
    "    'bagging_fraction': study_m2.best_trial.params['bagging_fraction'],\n",
    "    'seed': semillas[0],\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "train_data_m2 = lgb.Dataset(X_train_m2,\n",
    "                          label=y_train_binaria_m2,\n",
    "                          weight=w_train_m2)\n",
    "\n",
    "modelo_2 = lgb.train(params,\n",
    "                  train_data_m2,\n",
    "                  num_boost_round=best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Predigo Junio.\n",
    "print(\"Empieza el Modelo 1...\")\n",
    "y_pred_m1 = modelo_1.predict(X_test_m1)\n",
    "print(\"Empieza el Modelo 2...\")\n",
    "y_pred_m2 = modelo_2.predict(X_test_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Medimos las ganancias de los modelos.\n",
    "def ganancia_prob(y_pred, y_true, prop = 1):\n",
    "  ganancia = np.where(y_true == 1, ganancia_acierto, 0) - np.where(y_true == 0, costo_estimulo, 0)\n",
    "  return ganancia[y_pred >= 0.025].sum() / prop\n",
    "\n",
    "print(\"Ganancia M1:\", ganancia_prob(y_pred_m1, y_test_binaria_m1))\n",
    "print(\"Ganancia M2:\", ganancia_prob(y_pred_m2, y_test_binaria_m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. ¿Y si hacemos distintas divisiones entre Público y Privado?\n",
    "#i. Estratificamos 50 escenarios distintos.\n",
    "sss_futuro = StratifiedShuffleSplit(n_splits=50,\n",
    "                                    test_size=0.1,\n",
    "                                    random_state=semillas[0])\n",
    "\n",
    "#ii. Definimos los modelos y sus respectivos X_test y y_test.\n",
    "modelos = {\n",
    "    \"m1\": {\"pred\": y_pred_m1, \"X_test\": X_test_m1, \"y_test\": y_test_binaria_m1},\n",
    "    \"m2\": {\"pred\": y_pred_m2, \"X_test\": X_test_m2, \"y_test\": y_test_binaria_m2},\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "#iii. Para cada modelo, aplicamos StratifiedShuffleSplit.\n",
    "for name, data in modelos.items():\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    y_pred = data[\"pred\"]\n",
    "    \n",
    "    for private_index, public_index in sss_futuro.split(X_test, y_test):\n",
    "        row = {}\n",
    "        # Calculamos la ganancia en la división pública y privada.\n",
    "        row[name + \"_public\"] = ganancia_prob(y_pred[public_index], y_test.iloc[public_index], 0.1)\n",
    "        row[name + \"_private\"] = ganancia_prob(y_pred[private_index], y_test.iloc[private_index], 0.9)\n",
    "        rows.append(row)\n",
    "\n",
    "#iv. Convertimos a DataFrame.\n",
    "df_lb = pd.DataFrame(rows)\n",
    "\n",
    "#v. Transformación a formato largo para visualización.\n",
    "df_lb_long = df_lb.reset_index()\n",
    "df_lb_long = df_lb_long.melt(id_vars=['index'], var_name='model_type', value_name='ganancia')\n",
    "df_lb_long[['modelo', 'tipo']] = df_lb_long['model_type'].str.split('_', expand=True)\n",
    "df_lb_long = df_lb_long[['ganancia', 'tipo', 'modelo']]\n",
    "\n",
    "#vi. Visualización con seaborn.\n",
    "g = sns.FacetGrid(df_lb_long, col=\"tipo\", row=\"modelo\", aspect=2)\n",
    "g.map(sns.histplot, \"ganancia\", kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Entender como se hubiese comportado el Público y Privado a partir de distintos puntos de corte de cantidad de envíos.\n",
    "#i. Función.\n",
    "def analisis_comportamiento_kaggle_completo(semilla, desde, paso, cantidad, private = False,ganancia = None,y_pred = None, y_etiqueta = None):\n",
    "\n",
    "  df_cut_point = pd.DataFrame({'ganancia': ganancia, 'y_pred_lgm': y_pred})\n",
    "  df_cut_point['nro_envios'] = df_cut_point.reset_index().index\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "\n",
    "  private_idx, public_idx = train_test_split(df_cut_point.index, test_size=0.1, random_state=semilla, stratify=y_etiqueta)\n",
    "\n",
    "  df_cut_point['public'] = 0.0\n",
    "  df_cut_point.loc[public_idx, 'public'] = ganancia[public_idx] / 0.1\n",
    "  df_cut_point['public_cum'] = df_cut_point['public'].cumsum()\n",
    "\n",
    "  maximo_paso = desde + paso*cantidad\n",
    "  plt.plot(df_cut_point['nro_envios'][list(range(desde, maximo_paso + 1, paso))], df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))], label='Ganancia Pública Acumulada')\n",
    "  max_public_cum = df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))].max()\n",
    "  max_public_idx = df_cut_point['public_cum'][list(range(desde, maximo_paso + 1, paso))].idxmax()\n",
    "  plt.axvline(x=max_public_idx, color='g', linestyle='--', label=f'Máximo Ganancia Pública en {max_public_idx}')\n",
    "\n",
    "  if private:\n",
    "    df_cut_point['private'] = 0.0\n",
    "    df_cut_point.loc[private_idx, 'private'] = ganancia[private_idx] / 0.9\n",
    "    df_cut_point['private_cum'] = df_cut_point['private'].cumsum()\n",
    "    plt.plot(df_cut_point['nro_envios'][4000:20000], df_cut_point['private_cum'][4000:20000], label='Ganancia Privada Acumulada')\n",
    "    max_private_cum = df_cut_point['private_cum'][4000:20000].max()\n",
    "    max_private_idx = df_cut_point['private_cum'][4000:20000].idxmax()\n",
    "    plt.axvline(x=max_private_idx, color='r', linestyle='--', label=f'Máximo Ganancia Privada en {max_private_idx}')\n",
    "\n",
    "  plt.title('Curva de Ganancia Pública y Privada')\n",
    "  plt.xlabel('Número de envíos')\n",
    "  plt.ylabel('Ganancia Acumulada')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii. Calculo de ganancia acumulada.\n",
    "#a. Modelo 1.\n",
    "ganancia_m1 = np.where(y_test_binaria_m1 == 1, ganancia_acierto, 0) - np.where(y_test_binaria_m1 == 0, costo_estimulo, 0)\n",
    "\n",
    "idx_m1 = np.argsort(y_pred_m1)[::-1]\n",
    "\n",
    "ganancia_m1 = ganancia_m1[idx_m1]\n",
    "y_pred_m1 = y_pred_m1[idx_m1]\n",
    "\n",
    "ganancia_cum_m1 = np.cumsum(ganancia_m1)\n",
    "\n",
    "#b. Modelo 2.\n",
    "ganancia_m2 = np.where(y_test_binaria_m2 == 1, ganancia_acierto, 0) - np.where(y_test_binaria_m2 == 0, costo_estimulo, 0)\n",
    "\n",
    "idx_m2 = np.argsort(y_pred_m2)[::-1]\n",
    "\n",
    "ganancia_m2 = ganancia_m2[idx_m2]\n",
    "y_pred_m2 = y_pred_m2[idx_m2]\n",
    "\n",
    "ganancia_cum_m2 = np.cumsum(ganancia_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii. Ejecución para el modelo 1.\n",
    "analisis_comportamiento_kaggle_completo(semillas[0],9000,100,50,private=True,ganancia=ganancia_m1,y_pred=y_pred_m1,y_etiqueta=y_test_binaria_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iii. Ejecución para el modelo 2.\n",
    "analisis_comportamiento_kaggle_completo(semillas[0],4000,500,32,private=True,ganancia=ganancia_m2,y_pred=y_pred_m2,y_etiqueta=y_test_binaria_m2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
